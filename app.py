import streamlit as st
from pathlib import Path
import time

# LangChain / Transformers
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

PDF_PATH = "ãƒãƒªãƒ¼ãƒ»ãƒ›ã‚šãƒƒã‚¿ãƒ¼ã‚·ãƒªãƒ¼ã‚¹ã‚™ã®ç™»å ´äººç‰©ä¸€è¦§.pdf"
INDEX_DIR = Path("store")

st.title("ğŸ“š ãƒãƒªãƒã‚¿è³ªå•ã‚¢ãƒ—ãƒª")
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æº–å‚™
@st.cache_resource(show_spinner="Vector DB ã‚’æº–å‚™ä¸­â€¦")
def load_vectorstore():
    if (INDEX_DIR / "index.faiss").exists():
        return FAISS.load_local(
            folder_path=str(INDEX_DIR),
            embeddings=HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                encode_kwargs={"normalize_embeddings": True},
            ),
            allow_dangerous_deserialization=True
        )
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=50)
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        encode_kwargs={"normalize_embeddings": True},
    )
    vs = FAISS.from_documents(splits, embeddings)
    vs.save_local(INDEX_DIR)
    return vs

# LLMï¼ˆç„¡æ–™ãƒ¢ãƒ‡ãƒ«ï¼‰æº–å‚™
@st.cache_resource(show_spinner="LLM ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­â€¦")
def build_llm():
    tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-rw-1b")
    model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-rw-1b")
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
    )
    return HuggingFacePipeline(pipeline=pipe)

# æ¨è«–
if query:
    vs = load_vectorstore()
    retriever = vs.as_retriever(search_kwargs={"k": 6})
    llm = build_llm()
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

    with st.spinner("ğŸ§™â€â™€ï¸ è€ƒãˆä¸­â€¦"):
        t0 = time.time()
        result = qa_chain.invoke({"query": query})
        st.write("ğŸ§™ å›ç­”ï¼š", result["result"])
        st.caption(f"â±ï¸ {time.time()-t0:.1f} ç§’ã§å®Œäº†")