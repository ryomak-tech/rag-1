import streamlit as st
from pathlib import Path
import time

# LangChain / LlamaCpp
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA

# â–¼ è‡ªåˆ†ã®ç’°å¢ƒã«åˆã‚ã›ã¦
PDF_PATH = "ãƒãƒªãƒ¼ãƒ»ãƒãƒƒã‚¿ãƒ¼ã‚·ãƒªãƒ¼ã‚ºã®ç™»å ´äººç‰©ä¸€è¦§.pdf"
MODEL_PATH = "models/swallow-13b-instruct.Q4_K_S.gguf"
INDEX_DIR = Path("store")
N_GPU_LAYERS = 35  # Apple Siliconãªã‚‰35ãã‚‰ã„ã€ãªã‘ã‚Œã°0
N_THREADS = 8      # CPUã®ç‰©ç†ã‚³ã‚¢æ•°

st.title("ğŸ§™â€â™‚ï¸ ãƒãƒªãƒ¼ãƒãƒƒã‚¿ãƒ¼è³ªå•ã‚¢ãƒ—ãƒª")
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# ãƒ™ã‚¯ãƒˆãƒ«DBï¼ˆåˆå›ã¯ç”Ÿæˆï¼‰
@st.cache_resource(show_spinner="ğŸ” ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æº–å‚™ä¸­â€¦")
def load_vectorstore():
    if (INDEX_DIR / "index.faiss").exists():
        return FAISS.load_local(
            folder_path=str(INDEX_DIR),
            embeddings=HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                encode_kwargs={"normalize_embeddings": True}
            ),
            allow_dangerous_deserialization=True
        )
    
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        encode_kwargs={"normalize_embeddings": True}
    )

    vs = FAISS.from_documents(splits, embeddings)
    vs.save_local(INDEX_DIR)
    return vs

# LLMï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
@st.cache_resource(show_spinner="ğŸ§  ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­â€¦")
def build_llm():
    return LlamaCpp(
        model_path=MODEL_PATH,
        n_ctx=2048,
        max_tokens=256,
        temperature=0.2,
        n_threads=N_THREADS,
        n_batch=192,
        n_gpu_layers=N_GPU_LAYERS,
        verbose=False
    )

# æ¨è«–å®Ÿè¡Œ
if query:
    vs = load_vectorstore()
    retriever = vs.as_retriever(search_kwargs={"k": 6})
    llm = build_llm()
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

    with st.spinner("ğŸ§™â€â™€ï¸ è€ƒãˆä¸­â€¦"):
        t0 = time.time()
        result = qa.invoke({"query": query})
        st.success("ğŸª„ å›ç­”ï¼š" + result["result"])
        st.caption(f"â±ï¸ {time.time() - t0:.1f} ç§’ã§å®Œäº†")