import streamlit as st
from langchain_community.llms import HuggingFaceHub
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import time

st.title("ğŸ§  ãƒãƒªãƒã‚¿è³ªå•ã‚¢ãƒ—ãƒªï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰LLMï¼‰")

query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆStreamlit Cloudã®Secretsã«è¨­å®šï¼‰
HUGGINGFACE_API_TOKEN = st.secrets["HUGGINGFACEHUB_API_TOKEN"]

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
template = """
æ¬¡ã®è³ªå•ã«ã€å­ã©ã‚‚ã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {question}
ç­”ãˆ:
"""
prompt = PromptTemplate(input_variables=["question"], template=template)

# LLMè¨­å®š
llm = HuggingFaceHub(
    repo_id="google/flan-t5-large",  # flan-t5-base ã‚ˆã‚Šå®‰å®š
    model_kwargs={"temperature": 0.7, "max_length": 256},
    huggingfacehub_api_token=HUGGINGFACE_API_TOKEN,
    task="text2text-generation"  # â† ã“ã‚ŒãŒãªã„ã¨ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
)

chain = LLMChain(llm=llm, prompt=prompt)

# æ¨è«–
if query:
    with st.spinner("ğŸ§™â€â™‚ï¸ è€ƒãˆä¸­..."):
        t0 = time.time()
        try:
            answer = chain.run(query)
            st.success("ğŸ§™ å›ç­”ï¼š" + answer.strip())
            st.caption(f"â±ï¸ {time.time() - t0:.1f} ç§’ã§å®Œäº†")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")