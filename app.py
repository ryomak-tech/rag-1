from huggingface_hub import hf_hub_download
import streamlit as st
from pathlib import Path
import time

# ---- LangChain / llama-cpp imports ----
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.llms import LlamaCpp
from langchain.chains import RetrievalQA

# ---------- ã“ã“ã‚’è‡ªåˆ†ã®ç’°å¢ƒã«åˆã‚ã›ã¦ ---------- #
PDF_PATH   = "ãƒãƒªãƒ¼ãƒ»ãƒ›ã‚šãƒƒã‚¿ãƒ¼ã‚·ãƒªãƒ¼ã‚¹ã‚™ã®ç™»å ´äººç‰©ä¸€è¦§.pdf"
INDEX_DIR  = Path("store")                       # FAISS ä¿å­˜å…ˆ
MODEL_PATH = "models/swallow-13b-instruct.Q4_K_S.gguf"  # GGUF ãƒ•ã‚¡ã‚¤ãƒ«
N_GPU_LAYERS = 35    # GPU(ã¾ãŸã¯Apple Silicon) ãŒç„¡ã‘ã‚Œã° 0
N_THREADS     = 8     # ç‰©ç†ã‚³ã‚¢æ•°ã«åˆã‚ã›ã‚‹
# -------------------------------------------------- #

st.title("ğŸ“š ãƒãƒªãƒã‚¿è³ªå•ã‚¢ãƒ—ãƒª")
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# ----------- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆFAISSï¼‰ã‚’ç”¨æ„ ----------- #
@st.cache_resource(show_spinner="Vector DB ã‚’æº–å‚™ä¸­â€¦")
def load_vectorstore():
    """åˆå›ã¯ PDF â†’ åˆ†å‰² â†’ åŸ‹ã‚è¾¼ã¿ â†’ FAISS ä¿å­˜ã€‚2 å›ç›®ä»¥é™ã¯ãƒ­ãƒ¼ãƒ‰ã®ã¿ã€‚"""
    if (INDEX_DIR / "index.faiss").exists():
        return FAISS.load_local(
            folder_path=str(INDEX_DIR),
            embeddings=HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                encode_kwargs={"normalize_embeddings": True},
            ),
            allow_dangerous_deserialization=True       # â† ã“ã‚Œã‚’å¿˜ã‚Œã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
        )

    # ----- åˆå›ã ã‘èµ°ã‚‹é‡ã„å‡¦ç† -----
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=450, chunk_overlap=50
    )
    splits = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        encode_kwargs={"normalize_embeddings": True},
    )
    vs = FAISS.from_documents(splits, embeddings)
    vs.save_local(INDEX_DIR)
    return vs

# ----------- LlamaCpp ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨æ„ ----------- #
@st.cache_resource(show_spinner="LLM ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­â€¦")
def build_llm():
    model_path = hf_hub_download(
    repo_id="itsryoma/swallow-13b-local",  # ã‚ãªãŸã®HFãƒªãƒã‚¸ãƒˆãƒª
    filename="swallow-13b-instruct.Q4_K_S.gguf",
    subfolder="models"
    )
    return LlamaCpp(
        model_path=MODEL_PATH,
        n_ctx=2048,
        max_tokens=256,
        temperature=0.2,
        n_threads=N_THREADS,
        n_batch=192,
        n_gpu_layers=N_GPU_LAYERS,   # CPU-only ç’°å¢ƒãªã‚‰ 0
        verbose=False,
        stream=False                 # é€æ¬¡å‡ºåŠ›ã‚’ä½¿ã†ãªã‚‰ True
    )

# ------------------- æ¨è«– ------------------- #
if query:
    vs = load_vectorstore()
    retriever = vs.as_retriever(search_kwargs={"k": 6})

    llm = build_llm()
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff"
    )

    with st.spinner("ğŸ§™â€â™€ï¸ è€ƒãˆä¸­â€¦"):
        t0 = time.time()
        result = qa_chain.invoke({"query": query})
        st.write("ğŸ§™ å›ç­”ï¼š", result["result"])
        st.caption(f"â±ï¸ {time.time()-t0:.1f} ç§’ã§å®Œäº†")