import streamlit as st
import time
from langchain_community.llms import HuggingFaceHub
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ§  ãƒãƒªãƒã‚¿è³ªå•ã‚¢ãƒ—ãƒªï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰LLMï¼‰")

# å…¥åŠ›æ¬„
query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# secrets.toml ã«ä¿å­˜ã—ãŸ Hugging Face ã® API ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª­ã¿è¾¼ã¿
HUGGINGFACE_API_TOKEN = st.secrets["HUGGINGFACEHUB_API_TOKEN"]

# LLM ã‚’æº–å‚™
llm = HuggingFaceHub(
    repo_id="google/flan-t5-base",  # ç„¡æ–™ã§è»½ã„ãƒ¢ãƒ‡ãƒ«
    model_kwargs={"temperature": 0.7, "max_length": 256},
    huggingfacehub_api_token=HUGGINGFACE_API_TOKEN
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©
template = """
æ¬¡ã®è³ªå•ã«å­ã©ã‚‚ã§ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {question}
ç­”ãˆ:
"""
prompt = PromptTemplate(input_variables=["question"], template=template)

# LLMChain ã‚’ä½œæˆ
chain = LLMChain(llm=llm, prompt=prompt)

# å…¥åŠ›ãŒã‚ã‚Œã°å‡¦ç†å®Ÿè¡Œ
if query:
    with st.spinner("ğŸ§™â€â™‚ï¸ è€ƒãˆä¸­..."):
        t0 = time.time()
        answer = chain.run(query)
        st.success("ğŸ§™ å›ç­”ï¼š" + answer.strip())
        st.caption(f"â±ï¸ {time.time() - t0:.1f} ç§’ã§å®Œäº†")